/*
 * This file is part of the Score-P software (http://www.score-p.org)
 *
 * Copyright (c) 2009-2012,
 *    RWTH Aachen University, Germany
 *    Gesellschaft fuer numerische Simulation mbH Braunschweig, Germany
 *    Technische Universitaet Dresden, Germany
 *    University of Oregon, Eugene, USA
 *    Forschungszentrum Juelich GmbH, Germany
 *    German Research School for Simulation Sciences GmbH, Juelich/Aachen, Germany
 *    Technische Universitaet Muenchen, Germany
 *
 * See the COPYING file in the package base directory for details.
 *
 */

/****************************************************************************
**  SCALASCA    http://www.scalasca.org/                                   **
*****************************************************************************
**  Copyright (c) 1998-2011                                                **
**  Forschungszentrum Juelich GmbH, Juelich Supercomputing Centre          **
**                                                                         **
**  Copyright (c) 2010-2011                                                **
**  German Research School for Simulation Sciences GmbH,                   **
**  Laboratory for Parallel Programming                                    **
**                                                                         **
**  Copyright (c) 2003-2008                                                **
**  University of Tennessee, Innovative Computing Laboratory               **
**                                                                         **
**  See the file COPYRIGHT in the package base directory for details       **
****************************************************************************/


/**
 * @file  SCOREP_Mpi_P2p.c
 * @maintainer Daniel Lorenz <d.lorenz@fz-juelich.de>
 * @status     alpha
 * @ingroup    MPI_Wrapper
 *
 * @brief C interface wrappers for point-to-point communication
 */

#include <config.h>
#include "SCOREP_Mpi.h"

/**
 * internal array of statuses
 */
static MPI_Status* scorep_mpi_status_array = NULL;

/**
 * size of internal status array
 */
static int scorep_mpi_status_array_size = 0;

/**
 * Get a pointer to a status array of at least 'size' statuses
 * @param  size minimal requested size
 * @return pointer to status array
 */
static MPI_Status*
scorep_mpi_get_status_array( int size )
{
    if ( ( scorep_mpi_status_array_size == 0 )
         && ( size > 0 ) )
    {
        /* -- never used: initialize -- */
        scorep_mpi_status_array = malloc( size * sizeof( MPI_Status ) );
        if ( scorep_mpi_status_array == NULL )
        {
            UTILS_ERROR( SCOREP_ERROR_MEM_ALLOC_FAILED,
                         "We have UTILS_FATAL() to abort!" );
            abort();
        }
        scorep_mpi_status_array_size = size;
    }
    else
    if ( size > scorep_mpi_status_array_size )
    {
        /* -- not enough room: expand -- */
        scorep_mpi_status_array = realloc( scorep_mpi_status_array, size * sizeof( MPI_Status ) );
        if ( scorep_mpi_status_array == NULL )
        {
            UTILS_ERROR( SCOREP_ERROR_MEM_ALLOC_FAILED,
                         "We have UTILS_FATAL() to abort!" );
            abort();
        }
        scorep_mpi_status_array_size = size;
    }
    return scorep_mpi_status_array;
}

/**
 * @name Blocking
 * @{
 */
#if HAVE( DECL_PMPI_BSEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Bsend )
/**
 * Measurement wrapper for MPI_Bsend
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpSend.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'MPI_Bsend'
 * @li MPI send event
 * @li exit region 'MPI_Bsend'
 */
int
MPI_Bsend( void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm )
{
    int return_val;
  #if !defined( SCOREP_MPI_NO_HOOKS )
    uint64_t start_time_stamp;
  #endif

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int sz;

        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_BSEND ] );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
            SCOREP_MpiSend( dest, SCOREP_MPI_COMM_HANDLE( comm ),
                            tag, count * sz );
        }
        return_val = PMPI_Bsend( buf, count, datatype, dest, tag, comm );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Bsend( buf, count, datatype, dest, tag, comm, start_time_stamp, return_val );
        }
    #endif

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_BSEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Bsend( buf, count, datatype, dest, tag, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_RSEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Rsend )
/**
 * Measurement wrapper for MPI_Rsend
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpSend.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'MPI_Rsend'
 * @li MPI send event
 * @li exit region 'MPI_Rsend'
 */
int
MPI_Rsend( void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm )
{
    int return_val;
  #if !defined( SCOREP_MPI_NO_HOOKS )
    uint64_t start_time_stamp;
  #endif

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int sz;

        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_RSEND ] );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
            SCOREP_MpiSend( dest, SCOREP_MPI_COMM_HANDLE( comm ),
                            tag, count * sz );
        }
        return_val = PMPI_Rsend( buf, count, datatype, dest, tag, comm );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Rsend( buf, count, datatype, dest, tag, comm, start_time_stamp, return_val );
        }
    #endif

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_RSEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Rsend( buf, count, datatype, dest, tag, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_SEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Send )
/**
 * Measurement wrapper for MPI_Send
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpSend.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'MPI_Send'
 * @li MPI send event
 * @li exit region 'MPI_Send'
 */
int
MPI_Send( void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm )
{
    int return_val;
  #if !defined( SCOREP_MPI_NO_HOOKS )
    uint64_t start_time_stamp;
  #endif

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int sz;

        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_SEND ] );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
            SCOREP_MpiSend( dest, SCOREP_MPI_COMM_HANDLE( comm ),
                            tag, count * sz );
        }
        return_val = PMPI_Send( buf, count, datatype, dest, tag, comm );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Send( buf, count, datatype, dest, tag, comm, start_time_stamp, return_val );
        }
    #endif

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_SEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Send( buf, count, datatype, dest, tag, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_SSEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Ssend )
/**
 * Measurement wrapper for MPI_Ssend
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpSend.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'MPI_Ssend'
 * @li MPI send event
 * @li exit region 'MPI_Ssend'
 */
int
MPI_Ssend( void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm )
{
    int return_val;
  #if !defined( SCOREP_MPI_NO_HOOKS )
    uint64_t start_time_stamp;
  #endif

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int sz;

        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_SSEND ] );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
            SCOREP_MpiSend( dest, SCOREP_MPI_COMM_HANDLE( comm ),
                            tag, count * sz );
        }
        return_val = PMPI_Ssend( buf, count, datatype, dest, tag, comm );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Ssend( buf, count, datatype, dest, tag, comm, start_time_stamp, return_val );
        }
    #endif

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_SSEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Ssend( buf, count, datatype, dest, tag, comm );
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_RECV ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Recv
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'MPI_Recv'
 * @li MPI recv event
 * @li exit region 'MPI_Recv'
 */
int
MPI_Recv( void* buf,
          int count,
          MPI_Datatype datatype,
          int source, int tag,
          MPI_Comm comm,
          MPI_Status* status )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int        sz;
        uint64_t   start_time_stamp;
        MPI_Status mystatus;

        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_RECV ] );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        if ( status == MPI_STATUS_IGNORE )
        {
            status = &mystatus;
        }
        return_val = PMPI_Recv( buf, count, datatype, source, tag, comm, status );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Recv( buf, count, datatype, source, tag, comm, status, start_time_stamp, return_val );
        }
    #endif

        if ( source != MPI_PROC_NULL && return_val == MPI_SUCCESS )
        {
            PMPI_Type_size( datatype, &sz );
            PMPI_Get_count( status, datatype, &count );
            SCOREP_MpiRecv( status->MPI_SOURCE, SCOREP_MPI_COMM_HANDLE( comm ),
                            status->MPI_TAG, count * sz );
        }



        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_RECV ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Recv( buf, count, datatype, source, tag, comm, status );
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_PROBE ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Probe )
/**
 * Measurement wrapper for MPI_Probe
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 * It wraps the MPI_Probe call with enter and exit events.
 */
int
MPI_Probe( int source, int tag, MPI_Comm comm, MPI_Status* status )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_PROBE ] );

        return_val = PMPI_Probe( source, tag, comm, status );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_PROBE ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Probe( source, tag, comm, status );
    }

    return return_val;
}
#endif


#if HAVE( DECL_PMPI_SENDRECV ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Sendrecv )
/**
 * Measurement wrapper for MPI_Sendrecv
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpSendrecv.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * @li enter region 'MPI_Sendrecv'
 * @li MPI send event
 * @li MPI receive event
 * @li exit region 'MPI_Sendrecv'
 */
int
MPI_Sendrecv( void* sendbuf, int sendcount, MPI_Datatype sendtype, int dest, int sendtag, void* recvbuf, int recvcount, MPI_Datatype recvtype, int source, int recvtag, MPI_Comm comm, MPI_Status* status )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int        sendsz, recvsz;
        MPI_Status mystatus;

        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_SENDRECV ] );

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( sendtype, &sendsz );
            SCOREP_MpiSend( dest, SCOREP_MPI_COMM_HANDLE( comm ),
                            sendtag, sendcount * sendsz );
        }
        if ( status == MPI_STATUS_IGNORE )
        {
            status = &mystatus;
        }
        return_val = PMPI_Sendrecv( sendbuf, sendcount, sendtype, dest, sendtag, recvbuf, recvcount, recvtype, source, recvtag, comm, status );
        if ( source != MPI_PROC_NULL && return_val == MPI_SUCCESS )
        {
            PMPI_Type_size( recvtype, &recvsz );
            PMPI_Get_count( status, recvtype, &recvcount );
            SCOREP_MpiRecv( status->MPI_SOURCE, SCOREP_MPI_COMM_HANDLE( comm ),
                            status->MPI_TAG, recvcount * recvsz );
        }

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_SENDRECV ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Sendrecv( sendbuf, sendcount, sendtype, dest, sendtag, recvbuf, recvcount, recvtype, source, recvtag, comm, status );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_SENDRECV_REPLACE ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Sendrecv_replace )
/**
 * Measurement wrapper for MPI_Sendrecv_replace
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpSendrecv.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * @li enter region 'MPI_Sendrecv_replace'
 * @li MPI send event
 * @li MPI receive event
 * @li exit region 'MPI_Sendrecv_replace'
 */
int
MPI_Sendrecv_replace( void* buf, int count, MPI_Datatype datatype, int dest, int sendtag, int source, int recvtag, MPI_Comm comm, MPI_Status* status )
{
    int          return_val;
    int          sendcount = count, recvcount = count;
    MPI_Datatype sendtype  = datatype, recvtype = datatype;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int        sendsz, recvsz;
        MPI_Status mystatus;

        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_SENDRECV_REPLACE ] );

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( sendtype, &sendsz );
            SCOREP_MpiSend( dest, SCOREP_MPI_COMM_HANDLE( comm ),
                            sendtag, sendcount * sendsz );
        }
        if ( status == MPI_STATUS_IGNORE )
        {
            status = &mystatus;
        }
        return_val = PMPI_Sendrecv_replace( buf, count, datatype, dest, sendtag, source, recvtag, comm, status );
        if ( source != MPI_PROC_NULL && return_val == MPI_SUCCESS )
        {
            PMPI_Type_size( recvtype, &recvsz );
            PMPI_Get_count( status, recvtype, &recvcount );
            SCOREP_MpiRecv( status->MPI_SOURCE, SCOREP_MPI_COMM_HANDLE( comm ),
                            status->MPI_TAG, recvcount * sendsz );
        }

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_SENDRECV_REPLACE ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Sendrecv_replace( buf, count, datatype, dest, sendtag, source, recvtag, comm, status );
    }

    return return_val;
}
#endif

/**
 * @}
 * @name Non-blocking
 * @{
 */

#if HAVE( DECL_PMPI_IBSEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Ibsend )
/**
 * Measurement wrapper for MPI_Ibsend
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpIsend.w
 * @note C interface
 * @note Introduced with MPI 1
 * @ingroup p2p
 */
int
MPI_Ibsend( void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request* request )
{
    int return_val;

  #if !defined( SCOREP_MPI_NO_HOOKS )
    uint64_t start_time_stamp;
  #endif

    const int xnb_active = ( scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK );
    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int                 sz;
        SCOREP_MpiRequestId reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_IBSEND ] );

        #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
            if ( xnb_active )
            {
                SCOREP_MpiIsend( dest, SCOREP_MPI_COMM_HANDLE( comm ),
                                 tag, count * sz, reqid );
            }
            else
            {
                SCOREP_MpiSend( dest, SCOREP_MPI_COMM_HANDLE( comm ),
                                tag, count * sz );
            }
        }

        return_val = PMPI_Ibsend( buf, count, datatype, dest, tag, comm, request );
        if ( xnb_active && dest != MPI_PROC_NULL && return_val == MPI_SUCCESS )
        {
            scorep_mpi_request_create( *request, SCOREP_MPI_REQUEST_SEND,
                                       tag, dest, count * sz, datatype, comm, reqid );

        #if !defined( SCOREP_MPI_NO_HOOKS )
            if ( SCOREP_IS_MPI_HOOKS_ON )
            {
                SCOREP_Hooks_Post_MPI_Ibsend( buf, count, datatype, dest, tag, comm, request, start_time_stamp, return_val );
            }
        #endif
        }
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_IBSEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Ibsend( buf, count, datatype, dest, tag, comm, request );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_IRSEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Irsend )
/**
 * Measurement wrapper for MPI_Irsend
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpIsend.w
 * @note C interface
 * @note Introduced with MPI 1
 * @ingroup p2p
 */
int
MPI_Irsend( void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request* request )
{
    int return_val;

  #if !defined( SCOREP_MPI_NO_HOOKS )
    uint64_t start_time_stamp;
  #endif

    const int xnb_active = ( scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK );
    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int                 sz;
        SCOREP_MpiRequestId reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_IRSEND ] );

        #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
            if ( xnb_active )
            {
                SCOREP_MpiIsend( dest, SCOREP_MPI_COMM_HANDLE( comm ),
                                 tag, count * sz, reqid );
            }
            else
            {
                SCOREP_MpiSend( dest, SCOREP_MPI_COMM_HANDLE( comm ),
                                tag, count * sz );
            }
        }

        return_val = PMPI_Irsend( buf, count, datatype, dest, tag, comm, request );
        if ( xnb_active && dest != MPI_PROC_NULL && return_val == MPI_SUCCESS )
        {
            scorep_mpi_request_create( *request, SCOREP_MPI_REQUEST_SEND,
                                       tag, dest, count * sz, datatype, comm, reqid );

        #if !defined( SCOREP_MPI_NO_HOOKS )
            if ( SCOREP_IS_MPI_HOOKS_ON )
            {
                SCOREP_Hooks_Post_MPI_Irsend( buf, count, datatype, dest, tag, comm, request, start_time_stamp, return_val );
            }
        #endif
        }
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_IRSEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Irsend( buf, count, datatype, dest, tag, comm, request );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_ISEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Isend )
/**
 * Measurement wrapper for MPI_Isend
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpIsend.w
 * @note C interface
 * @note Introduced with MPI 1
 * @ingroup p2p
 */
int
MPI_Isend( void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request* request )
{
    int return_val;

  #if !defined( SCOREP_MPI_NO_HOOKS )
    uint64_t start_time_stamp;
  #endif

    const int xnb_active = ( scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK );
    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int                 sz;
        SCOREP_MpiRequestId reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_ISEND ] );

        #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
            if ( xnb_active )
            {
                SCOREP_MpiIsend( dest, SCOREP_MPI_COMM_HANDLE( comm ),
                                 tag, count * sz, reqid );
            }
            else
            {
                SCOREP_MpiSend( dest, SCOREP_MPI_COMM_HANDLE( comm ),
                                tag, count * sz );
            }
        }

        return_val = PMPI_Isend( buf, count, datatype, dest, tag, comm, request );
        if ( xnb_active && dest != MPI_PROC_NULL && return_val == MPI_SUCCESS )
        {
            scorep_mpi_request_create( *request, SCOREP_MPI_REQUEST_SEND,
                                       tag, dest, count * sz, datatype, comm, reqid );

        #if !defined( SCOREP_MPI_NO_HOOKS )
            if ( SCOREP_IS_MPI_HOOKS_ON )
            {
                SCOREP_Hooks_Post_MPI_Isend( buf, count, datatype, dest, tag, comm, request, start_time_stamp, return_val );
            }
        #endif
        }
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_ISEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Isend( buf, count, datatype, dest, tag, comm, request );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_ISSEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Issend )
/**
 * Measurement wrapper for MPI_Issend
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpIsend.w
 * @note C interface
 * @note Introduced with MPI 1
 * @ingroup p2p
 */
int
MPI_Issend( void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request* request )
{
    int return_val;

  #if !defined( SCOREP_MPI_NO_HOOKS )
    uint64_t start_time_stamp;
  #endif

    const int xnb_active = ( scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK );
    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int                 sz;
        SCOREP_MpiRequestId reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_ISSEND ] );

        #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
            if ( xnb_active )
            {
                SCOREP_MpiIsend( dest, SCOREP_MPI_COMM_HANDLE( comm ),
                                 tag, count * sz, reqid );
            }
            else
            {
                SCOREP_MpiSend( dest, SCOREP_MPI_COMM_HANDLE( comm ),
                                tag, count * sz );
            }
        }

        return_val = PMPI_Issend( buf, count, datatype, dest, tag, comm, request );
        if ( xnb_active && dest != MPI_PROC_NULL && return_val == MPI_SUCCESS )
        {
            scorep_mpi_request_create( *request, SCOREP_MPI_REQUEST_SEND,
                                       tag, dest, count * sz, datatype, comm, reqid );

        #if !defined( SCOREP_MPI_NO_HOOKS )
            if ( SCOREP_IS_MPI_HOOKS_ON )
            {
                SCOREP_Hooks_Post_MPI_Issend( buf, count, datatype, dest, tag, comm, request, start_time_stamp, return_val );
            }
        #endif
        }
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_ISSEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Issend( buf, count, datatype, dest, tag, comm, request );
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_IRECV ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Irecv
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Irecv( void*        buf,
           int          count,
           MPI_Datatype datatype,
           int          source,
           int          tag,
           MPI_Comm     comm,
           MPI_Request* request )
{
    uint64_t  start_time_stamp;
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    const int xnb_active       = ( scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_IRECV ] );
    }

  #if !defined( SCOREP_MPI_NO_HOOKS )
    if ( SCOREP_IS_MPI_HOOKS_ON )
    {
        start_time_stamp = SCOREP_GetLastTimeStamp();
    }
  #endif

    return_val = PMPI_Irecv( buf, count, datatype, source, tag, comm, request );

    if ( source != MPI_PROC_NULL && return_val == MPI_SUCCESS )
    {
        SCOREP_MpiRequestId reqid = scorep_mpi_get_request_id();
        int                 sz;
        PMPI_Type_size( datatype, &sz );

        if ( event_gen_active && xnb_active )
        {
            SCOREP_MpiIrecvRequest( reqid );
        }

        scorep_mpi_request_create( *request, SCOREP_MPI_REQUEST_RECV,
                                   tag, 0, count * sz, datatype, comm, reqid );
        #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Irecv( buf, count, datatype, source, tag, comm, request, start_time_stamp, return_val );
        }
    #endif
    }

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_IRECV ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_IPROBE ) && !defined( SCOREP_MPI_NO_EXTRA ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Iprobe )
/**
 * Measurement wrapper for MPI_Iprobe
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 * It wraps the MPI_Iprobe call with enter and exit events.
 */
int
MPI_Iprobe( int source, int tag, MPI_Comm comm, int* flag, MPI_Status* status )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_IPROBE ] );

        return_val = PMPI_Iprobe( source, tag, comm, flag, status );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_IPROBE ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Iprobe( source, tag, comm, flag, status );
    }

    return return_val;
}
#endif


#if HAVE( DECL_PMPI_WAIT ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Wait
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Wait( MPI_Request* request,
          MPI_Status*  status )
{
    const int           event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int                 return_val;
    MPI_Status          mystatus;
    scorep_mpi_request* orig_req;
    uint64_t            start_time_stamp;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_WAIT ] );
    }

  #if !defined( SCOREP_MPI_NO_HOOKS )
    if ( SCOREP_IS_MPI_HOOKS_ON )
    {
        start_time_stamp = SCOREP_GetLastTimeStamp();
    }
  #endif

    if ( status == MPI_STATUS_IGNORE )
    {
        status = &mystatus;
    }

    orig_req   = scorep_mpi_request_get( *request );
    return_val = PMPI_Wait( request, status );

  #if !defined( SCOREP_MPI_NO_HOOKS )
    if ( SCOREP_IS_MPI_HOOKS_ON )
    {
        SCOREP_Hooks_Post_MPI_Asynch_Complete_Blocking( orig_req, status, start_time_stamp );
    }
  #endif

    scorep_mpi_check_request( orig_req, status );

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_WAIT ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_WAITALL ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Waitall
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Waitall( int          count,
             MPI_Request* requests,
             MPI_Status*  array_of_statuses )
{
    const int           event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    scorep_mpi_request* orig_req;
    int                 i;
    int                 return_val;
    uint64_t            start_time_stamp;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_WAITALL ] );
    }

  #if !defined( SCOREP_MPI_NO_HOOKS )
    if ( SCOREP_IS_MPI_HOOKS_ON )
    {
        start_time_stamp = SCOREP_GetLastTimeStamp();
    }
  #endif

  #if HAVE( MPI_STATUSES_IGNORE )
    if ( array_of_statuses == MPI_STATUSES_IGNORE )
    {
        /* allocate status array for internal use */
        array_of_statuses = scorep_mpi_get_status_array( count );
    }
  #endif

    scorep_mpi_save_request_array( requests, count );

    return_val = PMPI_Waitall( count, requests, array_of_statuses );

    for ( i = 0; i < count; i++ )
    {
        orig_req = scorep_mpi_saved_request_get( i );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Asynch_Complete_Blocking( orig_req, &( array_of_statuses[ i ] ), start_time_stamp );
        }
    #endif

        scorep_mpi_check_request( orig_req, &( array_of_statuses[ i ] ) );
    }
    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_WAITALL ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_WAITANY ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Waitany
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Waitany( int          count,
             MPI_Request* requests,
             int*         index,
             MPI_Status*  status )
{
    const int           event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int                 return_val;
    const int           xnb_active = ( scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK );
    scorep_mpi_request* orig_req;
    MPI_Status          mystatus;
    uint64_t            start_time_stamp;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_WAITANY ] );
    }

  #if !defined( SCOREP_MPI_NO_HOOKS )
    if ( SCOREP_IS_MPI_HOOKS_ON )
    {
        start_time_stamp = SCOREP_GetLastTimeStamp();
    }
  #endif

    if ( status == MPI_STATUS_IGNORE )
    {
        status = &mystatus;
    }

    scorep_mpi_save_request_array( requests, count );
    return_val = PMPI_Waitany( count, requests, index, status );

    if ( event_gen_active && xnb_active )
    {
        int i;

        for ( i = 0; i < count; ++i )
        {
            orig_req = scorep_mpi_saved_request_get( i );

            if ( i == *index )
            {
                  #if !defined( SCOREP_MPI_NO_HOOKS )
                if ( SCOREP_IS_MPI_HOOKS_ON )
                {
                    SCOREP_Hooks_Post_MPI_Asynch_Complete_Blocking( orig_req, status, start_time_stamp );
                }
          #endif
                scorep_mpi_check_request( orig_req, status );
            }
            else if ( orig_req )
            {
                SCOREP_MpiRequestTested( orig_req->id );
            }
        }
    }
    else
    {
        orig_req = scorep_mpi_saved_request_get( *index );
      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Asynch_Complete_Blocking( orig_req, status, start_time_stamp );
        }
      #endif
        scorep_mpi_check_request( orig_req, status );
    }

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_WAITANY ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_WAITSOME ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Waitsome
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Waitsome( int          incount,
              MPI_Request* array_of_requests,
              int*         outcount,
              int*         array_of_indices,
              MPI_Status*  array_of_statuses )
{
    const int           event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int                 return_val;
    const int           xnb_active = ( scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK );
    int                 i;
    scorep_mpi_request* orig_req;
    uint64_t            start_time_stamp;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_WAITSOME ] );
    }

  #if !defined( SCOREP_MPI_NO_HOOKS )
    if ( SCOREP_IS_MPI_HOOKS_ON )
    {
        start_time_stamp = SCOREP_GetLastTimeStamp();
    }
  #endif

  #if HAVE( MPI_STATUSES_IGNORE )
    if ( array_of_statuses == MPI_STATUSES_IGNORE )
    {
        /* allocate status array for internal use */
        array_of_statuses = scorep_mpi_get_status_array( incount );
    }
  #endif

    scorep_mpi_save_request_array( array_of_requests, incount );

    return_val = PMPI_Waitsome( incount, array_of_requests, outcount,
                                array_of_indices, array_of_statuses );
    if ( event_gen_active && xnb_active )
    {
        int        j, tmp, cur;
        MPI_Status tmpstat;

        cur = 0;

        for ( i = 0; i < incount; ++i )
        {
            orig_req = scorep_mpi_saved_request_get( i );

            if ( orig_req )
            {
                for ( j = cur; j < *outcount && i != array_of_indices[ j ]; ++j )
                {
                    ;
                }

                if ( j < *outcount )
                {
                    tmpstat = array_of_statuses[ cur ];
                  #if !defined( SCOREP_MPI_NO_HOOKS )
                    if ( SCOREP_IS_MPI_HOOKS_ON )
                    {
                        SCOREP_Hooks_Post_MPI_Asynch_Complete_Blocking( orig_req, &( array_of_statuses[ cur ] ), start_time_stamp );
                    }
                          #endif
                    scorep_mpi_check_request( orig_req, &( array_of_statuses[ cur ] ) );
                    array_of_statuses[ j ] = tmpstat;

                    tmp                     = array_of_indices[ cur ];
                    array_of_indices[ cur ] = array_of_indices[ j ];
                    array_of_indices[ j ]   = tmp;

                    ++cur;
                }
                else
                {
                    SCOREP_MpiRequestTested( orig_req->id );
                }
            }
        }
    }
    else
    {
        for ( i = 0; i < *outcount; ++i )
        {
            orig_req = scorep_mpi_saved_request_get( array_of_indices[ i ] );
                  #if !defined( SCOREP_MPI_NO_HOOKS )
            if ( SCOREP_IS_MPI_HOOKS_ON )
            {
                SCOREP_Hooks_Post_MPI_Asynch_Complete_Blocking( orig_req, &( array_of_statuses[ i ] ), start_time_stamp );
            }
          #endif
            scorep_mpi_check_request( orig_req, &( array_of_statuses[ i ] ) );
        }
    }

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_WAITSOME ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_TEST ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Test
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Test( MPI_Request* request,
          int*         flag,
          MPI_Status*  status )
{
    const int           event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int                 return_val;
    const int           xtest_active = ( scorep_mpi_enabled & SCOREP_MPI_ENABLED_XREQTEST );
    scorep_mpi_request* orig_req;
    MPI_Status          mystatus;
    uint64_t            start_time_stamp;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_TEST ] );
    }

  #if !defined( SCOREP_MPI_NO_HOOKS )
    if ( SCOREP_IS_MPI_HOOKS_ON )
    {
        start_time_stamp = SCOREP_GetLastTimeStamp();
    }
  #endif

    if ( status == MPI_STATUS_IGNORE )
    {
        status = &mystatus;
    }
    orig_req   = scorep_mpi_request_get( *request );
    return_val = PMPI_Test( request, flag, status );
    if ( *flag )
    {
          #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Asynch_Complete( orig_req, status, start_time_stamp );
        }
      #endif
        scorep_mpi_check_request( orig_req, status );
    }
    else if ( orig_req && event_gen_active && xtest_active )
    {
        SCOREP_MpiRequestTested( orig_req->id );
    }

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_TEST ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_TESTANY ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Testany
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Testany( int          count,
             MPI_Request* array_of_requests,
             int*         index,
             int*         flag,
             MPI_Status*  status )
{
    const int           event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int                 return_val;
    const int           xtest_active = ( scorep_mpi_enabled & SCOREP_MPI_ENABLED_XREQTEST );
    scorep_mpi_request* orig_req;
    MPI_Status          mystatus;
    uint64_t            start_time_stamp;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_TESTANY ] );
    }

  #if !defined( SCOREP_MPI_NO_HOOKS )
    if ( SCOREP_IS_MPI_HOOKS_ON )
    {
        start_time_stamp = SCOREP_GetLastTimeStamp();
    }
  #endif

    if ( status == MPI_STATUS_IGNORE )
    {
        status = &mystatus;
    }
    scorep_mpi_save_request_array( array_of_requests, count );
    return_val = PMPI_Testany( count, array_of_requests, index, flag, status );

    if ( event_gen_active && xtest_active )
    {
        int i;

        for ( i = 0; i < count; ++i )
        {
            orig_req = scorep_mpi_saved_request_get( i );

            if ( *index == i )
            {
          #if !defined( SCOREP_MPI_NO_HOOKS )
                if ( SCOREP_IS_MPI_HOOKS_ON )
                {
                    SCOREP_Hooks_Post_MPI_Asynch_Complete( orig_req, status, start_time_stamp );
                }
          #endif
                scorep_mpi_check_request( orig_req, status );
            }
            else if ( orig_req )
            {
                SCOREP_MpiRequestTested( orig_req->id );
            }
        }
    }
    else if ( *flag && *index != MPI_UNDEFINED )
    {
        orig_req = scorep_mpi_saved_request_get( *index );
      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Asynch_Complete( orig_req, status, start_time_stamp );
        }
      #endif
        scorep_mpi_check_request( orig_req, status );
    }
    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_TESTANY ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_TESTALL ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Testall
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Testall( int          count,
             MPI_Request* array_of_requests,
             int*         flag,
             MPI_Status*  array_of_statuses )
{
    const int           event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int                 return_val;
    const int           xtest_active = ( scorep_mpi_enabled & SCOREP_MPI_ENABLED_XREQTEST );
    int                 i;
    scorep_mpi_request* orig_req;
    uint64_t            start_time_stamp;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_TESTALL ] );
    }

  #if !defined( SCOREP_MPI_NO_HOOKS )
    if ( SCOREP_IS_MPI_HOOKS_ON )
    {
        start_time_stamp = SCOREP_GetLastTimeStamp();
    }
  #endif

  #if HAVE( MPI_STATUSES_IGNORE )
    if ( array_of_statuses == MPI_STATUSES_IGNORE )
    {
        /* allocate status array for internal use */
        array_of_statuses = scorep_mpi_get_status_array( count );
    }
  #endif

    scorep_mpi_save_request_array( array_of_requests, count );

    return_val = PMPI_Testall( count, array_of_requests, flag, array_of_statuses );

    if ( *flag )
    {
        for ( i = 0; i < count; i++ )
        {
            orig_req = scorep_mpi_saved_request_get( i );
          #if !defined( SCOREP_MPI_NO_HOOKS )
            if ( SCOREP_IS_MPI_HOOKS_ON )
            {
                SCOREP_Hooks_Post_MPI_Asynch_Complete( orig_req, &( array_of_statuses[ i ] ), start_time_stamp );
            }
          #endif
            scorep_mpi_check_request( orig_req, &( array_of_statuses[ i ] ) );
        }
    }
    else if ( event_gen_active && xtest_active )
    {
        int i;

        for ( i = 0; i < count; i++ )
        {
            orig_req = scorep_mpi_saved_request_get( i );
            if ( orig_req )
            {
                SCOREP_MpiRequestTested( orig_req->id );
            }
        }
    }
    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_TESTALL ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_TESTSOME ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Testsome
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Testsome( int          incount,
              MPI_Request* array_of_requests,
              int*         outcount,
              int*         array_of_indices,
              MPI_Status*  array_of_statuses )
{
    const int           event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int                 return_val;
    const int           xtest_active = ( scorep_mpi_enabled & SCOREP_MPI_ENABLED_XREQTEST );
    int                 i;
    scorep_mpi_request* orig_req;
    uint64_t            start_time_stamp;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_TESTSOME ] );
    }

  #if !defined( SCOREP_MPI_NO_HOOKS )
    if ( SCOREP_IS_MPI_HOOKS_ON )
    {
        start_time_stamp = SCOREP_GetLastTimeStamp();
    }
  #endif

  #if HAVE( MPI_STATUSES_IGNORE )
    if ( array_of_statuses == MPI_STATUSES_IGNORE )
    {
        /* allocate status array for internal use */
        array_of_statuses = scorep_mpi_get_status_array( incount );
    }
  #endif

    scorep_mpi_save_request_array( array_of_requests, incount );
    return_val = PMPI_Testsome( incount, array_of_requests, outcount,
                                array_of_indices, array_of_statuses );

    if ( event_gen_active && xtest_active )
    {
        int        cur, j, tmp;
        MPI_Status tmpstat;

        cur = 0;

        for ( i = 0; i < incount; ++i )
        {
            orig_req = scorep_mpi_saved_request_get( i );

            if ( orig_req )
            {
                for ( j = cur; j < *outcount && i != array_of_indices[ j ]; ++j )
                {
                    ;
                }

                if ( j < *outcount )
                {
                    tmpstat = array_of_statuses[ cur ];
                  #if !defined( SCOREP_MPI_NO_HOOKS )
                    if ( SCOREP_IS_MPI_HOOKS_ON )
                    {
                        SCOREP_Hooks_Post_MPI_Asynch_Complete( orig_req, &( array_of_statuses[ cur ] ), start_time_stamp );
                    }
                          #endif
                    scorep_mpi_check_request( orig_req, &( array_of_statuses[ cur ] ) );
                    array_of_statuses[ j ] = tmpstat;

                    tmp                     = array_of_indices[ cur ];
                    array_of_indices[ cur ] = array_of_indices[ j ];
                    array_of_indices[ j ]   = tmp;

                    ++cur;
                }
                else
                {
                    SCOREP_MpiRequestTested( orig_req->id );
                }
            }
        }
    }
    else
    {
        for ( i = 0; i < *outcount; ++i )
        {
            orig_req = scorep_mpi_saved_request_get( array_of_indices[ i ] );
                  #if !defined( SCOREP_MPI_NO_HOOKS )
            if ( SCOREP_IS_MPI_HOOKS_ON )
            {
                SCOREP_Hooks_Post_MPI_Asynch_Complete( orig_req, &( array_of_statuses[ i ] ), start_time_stamp );
            }
          #endif
            scorep_mpi_check_request( orig_req, &( array_of_statuses[ i ] ) );
        }
    }

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_TESTSOME ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

/**
 * @}
 * @name Persitent requests
 * @{
 */

#if HAVE( DECL_PMPI_BSEND_INIT ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Bsend_init )
/**
 * Measurement wrapper for MPI_Bsend_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpSendinit.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'MPI_Bsend_init'
 * @li exit region 'MPI_Bsend_init'
 */
int
MPI_Bsend_init( void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request* request )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int       return_val, sz;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_BSEND_INIT ] );
    }

    PMPI_Type_size( datatype, &sz );

    return_val = PMPI_Bsend_init( buf, count, datatype, dest, tag, comm, request );
    if ( dest != MPI_PROC_NULL && return_val == MPI_SUCCESS )
    {
        scorep_mpi_request_create( *request, ( SCOREP_MPI_REQUEST_SEND | SCOREP_MPI_REQUEST_IS_PERSISTENT ),
                                   tag, dest, count * sz, datatype, comm,
                                   scorep_mpi_get_request_id() );
    }
    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_BSEND_INIT ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_RSEND_INIT ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Rsend_init )
/**
 * Measurement wrapper for MPI_Rsend_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpSendinit.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'MPI_Rsend_init'
 * @li exit region 'MPI_Rsend_init'
 */
int
MPI_Rsend_init( void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request* request )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int       return_val, sz;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_RSEND_INIT ] );
    }

    PMPI_Type_size( datatype, &sz );

    return_val = PMPI_Rsend_init( buf, count, datatype, dest, tag, comm, request );
    if ( dest != MPI_PROC_NULL && return_val == MPI_SUCCESS )
    {
        scorep_mpi_request_create( *request, ( SCOREP_MPI_REQUEST_SEND | SCOREP_MPI_REQUEST_IS_PERSISTENT ),
                                   tag, dest, count * sz, datatype, comm,
                                   scorep_mpi_get_request_id() );
    }
    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_RSEND_INIT ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_SEND_INIT ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Send_init )
/**
 * Measurement wrapper for MPI_Send_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpSendinit.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'MPI_Send_init'
 * @li exit region 'MPI_Send_init'
 */
int
MPI_Send_init( void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request* request )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int       return_val, sz;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_SEND_INIT ] );
    }

    PMPI_Type_size( datatype, &sz );

    return_val = PMPI_Send_init( buf, count, datatype, dest, tag, comm, request );
    if ( dest != MPI_PROC_NULL && return_val == MPI_SUCCESS )
    {
        scorep_mpi_request_create( *request, ( SCOREP_MPI_REQUEST_SEND | SCOREP_MPI_REQUEST_IS_PERSISTENT ),
                                   tag, dest, count * sz, datatype, comm,
                                   scorep_mpi_get_request_id() );
    }
    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_SEND_INIT ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_SSEND_INIT ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Ssend_init )
/**
 * Measurement wrapper for MPI_Ssend_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpSendinit.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'MPI_Ssend_init'
 * @li exit region 'MPI_Ssend_init'
 */
int
MPI_Ssend_init( void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request* request )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int       return_val, sz;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_SSEND_INIT ] );
    }

    PMPI_Type_size( datatype, &sz );

    return_val = PMPI_Ssend_init( buf, count, datatype, dest, tag, comm, request );
    if ( dest != MPI_PROC_NULL && return_val == MPI_SUCCESS )
    {
        scorep_mpi_request_create( *request, ( SCOREP_MPI_REQUEST_SEND | SCOREP_MPI_REQUEST_IS_PERSISTENT ),
                                   tag, dest, count * sz, datatype, comm,
                                   scorep_mpi_get_request_id() );
    }
    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_SSEND_INIT ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_RECV_INIT ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Recv_init
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Recv_init( void*        buf,
               int          count,
               MPI_Datatype datatype,
               int          source,
               int          tag,
               MPI_Comm     comm,
               MPI_Request* request )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_RECV_INIT ] );
    }

    return_val = PMPI_Recv_init( buf, count, datatype, source, tag, comm, request );
    if ( source != MPI_PROC_NULL && return_val == MPI_SUCCESS )
    {
        int sz;
        PMPI_Type_size( datatype, &sz );
        scorep_mpi_request_create( *request, ( SCOREP_MPI_REQUEST_RECV | SCOREP_MPI_REQUEST_IS_PERSISTENT ),
                                   tag, source, count * sz, datatype, comm,
                                   scorep_mpi_get_request_id() );

        #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Recv_init( buf, count, datatype, source, tag, comm, request, 0, return_val );
        }
    #endif
    }

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_RECV_INIT ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_START ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Start
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Start( MPI_Request* request )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    const int xnb_active       = ( scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK );
    int       return_val;
    uint64_t  start_time_stamp;

    if ( event_gen_active )
    {
        scorep_mpi_request* req;

        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_START ] );

        #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        req = scorep_mpi_request_get( *request );
        if ( req && ( req->flags & SCOREP_MPI_REQUEST_IS_PERSISTENT ) )
        {
            req->flags |= SCOREP_MPI_REQUEST_IS_ACTIVE;
            if ( ( req->flags & SCOREP_MPI_REQUEST_SEND ) && ( req->dest != MPI_PROC_NULL ) )
            {
                if ( xnb_active )
                {
                    SCOREP_MpiIsend( req->dest, req->comm_handle,
                                     req->tag, req->bytes, req->id );
                }
                else
                {
                    SCOREP_MpiSend( req->dest, req->comm_handle,
                                    req->tag, req->bytes );
                }
            }
            else if ( req->flags & SCOREP_MPI_REQUEST_RECV && xnb_active )
            {
                SCOREP_MpiIrecvRequest( req->id );
            }
        }
    }

    return_val = PMPI_Start( request );

  #if !defined( SCOREP_MPI_NO_HOOKS )
    if ( SCOREP_IS_MPI_HOOKS_ON )
    {
        SCOREP_Hooks_Post_MPI_Start( request, start_time_stamp, return_val );
    }
  #endif
    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_START ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_STARTALL ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Startall
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Startall( int          count,
              MPI_Request* array_of_requests )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    const int xnb_active       = ( scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK );
    int       return_val, i;
    uint64_t  start_time_stamp;

    if ( event_gen_active )
    {
        MPI_Request* request;

        scorep_mpi_request* req;

        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_STARTALL ] );

        #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        for ( i = 0; i < count; i++ )
        {
            request = &array_of_requests[ i ];
            req     = scorep_mpi_request_get( *request );

            if ( req && ( req->flags & SCOREP_MPI_REQUEST_IS_PERSISTENT ) )
            {
                req->flags |= SCOREP_MPI_REQUEST_IS_ACTIVE;
                if ( ( req->flags & SCOREP_MPI_REQUEST_SEND ) && ( req->dest != MPI_PROC_NULL ) )
                {
                    SCOREP_MpiIsend( req->dest, req->comm_handle,
                                     req->tag, req->bytes, req->id );
                }
                else if ( req->flags & SCOREP_MPI_REQUEST_RECV && xnb_active )
                {
                    SCOREP_MpiIrecvRequest( req->id );
                }
            }
        }
    }

    return_val = PMPI_Startall( count, array_of_requests );

  #if !defined( SCOREP_MPI_NO_HOOKS )
    if ( SCOREP_IS_MPI_HOOKS_ON )
    {
        for ( i = 0; i < count; i++ )
        {
            SCOREP_Hooks_Post_MPI_Start( &array_of_requests[ i ], start_time_stamp, return_val );
        }
    }
  #endif

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_STARTALL ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_REQUEST_FREE ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Request_free
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Request_free( MPI_Request* request )
{
    const int           event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    const int           xnb_active       = ( scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK );
    int                 orig_req_null    = ( *request == MPI_REQUEST_NULL );
    int                 return_val;
    scorep_mpi_request* req;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_REQUEST_FREE ] );
    }

    req = scorep_mpi_request_get( *request );
  #if !defined( SCOREP_MPI_NO_HOOKS )
    if ( SCOREP_IS_MPI_HOOKS_ON )
    {
        SCOREP_Hooks_Pre_MPI_Request_free( req );
    }
  #endif
    if ( req )
    {
        if ( req->flags & SCOREP_MPI_REQUEST_CAN_CANCEL && event_gen_active && xnb_active )
        {
            MPI_Status status;
            int        cancelled;
            /* -- Must check if request was cancelled and write the
             *    cancel event. Not doing so will confuse the trace
             *    analysis.
             */
            return_val = PMPI_Wait( request, &status );
            PMPI_Test_cancelled( &status, &cancelled );

            if ( cancelled )
            {
                SCOREP_MpiRequestCancelled( req->id );
            }
        }

        if ( ( req->flags & SCOREP_MPI_REQUEST_IS_PERSISTENT ) && ( req->flags & SCOREP_MPI_REQUEST_IS_ACTIVE ) )
        {
            /* mark active requests for deallocation */
            req->flags |= SCOREP_MPI_REQUEST_DEALLOCATE;
        }
        else
        {
            /* deallocate inactive requests -*/
            scorep_mpi_request_free( req );
        }
    }

    /* -- We had to call PMPI_Wait for cancellable requests, which already
     *    frees (non-persistent) requests itself and sets them to
     *    MPI_REQUEST_NULL.
     *    As MPI_Request_free does not really like being called with
     *    MPI_REQUEST_NULL, we have to catch this situation here and only
     *    pass MPI_REQUEST_NULL if the application explicitely wanted that
     *    for some reason.
     */
    if ( *request != MPI_REQUEST_NULL || orig_req_null )
    {
        return_val = PMPI_Request_free( request );
    }


    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_REQUEST_FREE ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_CANCEL ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Cancel
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Cancel( MPI_Request* request )
{
    const int           event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int                 return_val;
    scorep_mpi_request* req;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_CANCEL ] );
    }

    /* Mark request as cancellable and check for successful cancellation
     * on request completion or MPI_Request_free.
     * If XNONBLOCK is enabled, there will be a 'cancelled' event
     * instead of a normal completion event in the trace, which can be
     * checked for by the trace analysis.
     */

    req = scorep_mpi_request_get( *request );

    if ( req )
    {
        req->flags |= SCOREP_MPI_REQUEST_CAN_CANCEL;
    }

    return_val = PMPI_Cancel( request );

  #if !defined( SCOREP_MPI_NO_HOOKS )
    if ( SCOREP_IS_MPI_HOOKS_ON )
    {
        SCOREP_Hooks_Post_MPI_Cancel( req );
    }
  #endif

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_CANCEL ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_TEST_CANCELLED ) && !defined( SCOREP_MPI_NO_EXTRA ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Test_cancelled )
/**
 * Measurement wrapper for MPI_Test_cancelled
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 * It wraps the MPI_Test_cancelled call with enter and exit events.
 */
int
MPI_Test_cancelled( MPI_Status* status, int* flag )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_TEST_CANCELLED ] );

        return_val = PMPI_Test_cancelled( status, flag );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_TEST_CANCELLED ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Test_cancelled( status, flag );
    }

    return return_val;
}
#endif


/**
 * @}
 * @name Auxiluary functions
 * @{
 */

#if HAVE( DECL_PMPI_BUFFER_ATTACH ) && !defined( SCOREP_MPI_NO_EXTRA ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Buffer_attach )
/**
 * Measurement wrapper for MPI_Buffer_attach
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 * It wraps the MPI_Buffer_attach call with enter and exit events.
 */
int
MPI_Buffer_attach( void* buffer, int size )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_BUFFER_ATTACH ] );

        return_val = PMPI_Buffer_attach( buffer, size );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_BUFFER_ATTACH ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Buffer_attach( buffer, size );
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_BUFFER_DETACH ) && !defined( SCOREP_MPI_NO_EXTRA ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Buffer_detach )
/**
 * Measurement wrapper for MPI_Buffer_detach
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 * It wraps the MPI_Buffer_detach call with enter and exit events.
 */
int
MPI_Buffer_detach( void* buffer, int* size )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_BUFFER_DETACH ] );

        return_val = PMPI_Buffer_detach( buffer, size );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_BUFFER_DETACH ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Buffer_detach( buffer, size );
    }

    return return_val;
}
#endif


/**
 * @}
 */
